Introduction
Large language models (LLMs) have achieved remarkable levels of success in various natural language processing (NLP) applications, including question-answering, entity extraction, and summarization. This has been made possible, in large part, by the introduction of the transformer architecture, which can leverage large amounts of unlabeled, unstructured data, enabling the scaling of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a self-supervised pre-training phase, followed by supervised alignment tuning phases.
The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a model is trained in an auto-regressive manner to predict the next token in the target language using trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a time. Alignment tuning, typically happens in two stages: instruction tuning, followed by preference tuning. Instruction tuning is more akin to the traditional model training approach in machine learning, where the model is trained directly on tasks of interest. In this stage, the model is given a task description in the form of a natural language instruction (e.g., "Summarize the following news article in 2 lines: {News article}") and the model is trained to maximize the likelihood of the provided ground truth summary. Preference tuning, on the other hand, is done using techniques such as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.
In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small fraction of the overall training procedure, both in terms of the data used as well as the compute infrastructure required to train models (Touvron et al., 2023). For example, Meta’s LLaMA 2 models were trained with just tens of thousands of high-quality human-generated instruction/response data pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as compared to pretraining data volumes (Touvron et al., 2023). From a traditional machine learning training perspective, this imbalance in the scale across the phases is unconventional—typically one would expect a model to perform best when it has been trained directly on the desired tasks, using as much data as possible. The deviation from the traditional LLM approach relies on the idea that pre-training captures enough of the distribution of language and knowledge, such that a small amount of supervised training can “unlock” or shape latent abilities related to the ultimate desired instruction-following behavior of the model. However, unlike the unstructured data that is abundantly available in the public domain, high-quality, human-generated task-specific instruction data is costly to procure, even via crowdsourcing, and human-generated instruction data is typically closely guarded by model builders, even for ostensibly “open” model-building efforts.
In this work, we address the challenges associated with scaling of the alignment-tuning phase and propose a new method called LAB: Large-scale Alignment for chatBots. The LAB method consists of two components: (i) a taxonomy-guided synthetic data generation method and quality assurance process that yields a highly diverse and high-quality instruction dataset, without resorting to the use of proprietary LLMs like GPT-4 or substantial human curation, and (ii) a novel multi-phase training framework and unconventional tuning regime that allows for adding new knowledge and instruction-following abilities into pre-trained LLMs without suffering from catastrophic forgetting. Our findings show that LAB-trained models can perform competitively with proprietary and open-source models that use human annotations and/or synthetic data generated using GPT-4 on a number of benchmarks.


Related Work
Existing methods for instruction tuning typically either rely on humans for generating high-quality datasets or use synthetic data generation using a large teacher model. OpenAI (Ouyang et al., 2022) arguably set the standard for model alignment from human data, employing human annotators to gather data for supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) training. Collecting human-generated data for these steps is a complex undertaking; the selection of annotators requires a rigorous multi-stage screening process aimed at achieving high inter-annotator agreement, and collecting even modest amounts of data (by LLM standards) requires the coordination of large groups of annotators. The creators of the LLaMA 2 model series (Touvron et al., 2023) followed a similar recipe, collecting tens of thousands of human-generated instruction samples and approximately 1 million human-annotated binary comparisons for reward modeling. Not only are such approaches expensive and time-consuming, but they can also potentially limit agility in exploring the space of instructions and capabilities the model is trained to perform. Alternatives to this approach, such as transforming existing human datasets into instructions via templating (Wei et al.) can be more cost-effective but face limitations in the naturalness and length of the responses used for training.
More recently, training with synthetic data generated from LLMs has emerged as an alternative to purely human-data-based approaches. Wang et al. (2023) introduced Self-Instruct, which leverages a small number of handwritten human seed instructions as input to a bootstrapping process to generate a large number of samples using an LLM’s own generation abilities. Taori et al. (2023) built upon Self-Instruct, using a larger teacher model to generate synthetic data to train a smaller student model, and incorporating principles in the generation prompt to promote diversity in the generated instruction data. Xu et al. (2023) introduced Evol-Instruct, another variant of Self-Instruct, that synthesizes iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee et al. (2023) and Mitra et al. (2023) present a synthetic data generation approach to enhance task diversity and scalability, alongside a progressive training framework aimed at improving the model’s reasoning ability and response style to match teacher models. This is achieved by generating rich reasoning signals in the generated answer and progressively training on datasets of varying difficulty in incremental phases.
Similar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data from a teacher model. However, unlike LAB, GLAN cannot be used to generate synthetic data from domains that are not captured in the teacher model’s support. As such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses complicated questions about the usability of generated data (especially for commercial purposes) since the terms of use of proprietary models typically forbid using the model to improve other models.


Methodology
LAB consists of two components: (i) a taxonomy to enable data curation (section 3.1) as well as guide the synthetic data generator (section 3.2) and (ii) a multi-phased instruction-tuning method with replay buffers to enable large-scale alignment-tuning (section 3.3). (i) serves the purpose of ensuring high diversity and quality in the synthetically generated instruction-tuning dataset while (ii) ensures training stability and prevents catastrophic forgetting. Figure 1 provides an overview of the end-to-end pipeline of applying the LAB method to align a pre-trained LLM.

Taxonomy
To enable the data curator or the model designer to organize the instruction-tuning training data, we define a taxonomy that hierarchically classifies the data samples into smaller task groups. At a high level, the taxonomy has three main branches: knowledge, foundational skills, and compositional skills. Each of these branches is further split into more granular levels where the tasks are defined in the leaf nodes and exemplified by providing manually written instruction-response pairs. This allows for easily identifying missing tasks in the target LLM and other tasks of interest and adding them to the training data pool. New tasks are added to the taxonomy by creating a leaf node under the appropriate branch and attaching 1–3 examples.
Design and Structure
The design of the taxonomy is central to LAB’s effectiveness. The taxonomy is structured hierarchically to capture the full spectrum of tasks that an LLM might encounter. At the top level, we have three main branches: knowledge, foundational skills, and compositional skills. These branches are further subdivided into more specific categories, each represented by leaf nodes containing examples of instruction-response pairs.
For instance, the knowledge branch might include domains such as finance, statistics, and medicine, each with its own set of subdomains and associated tasks. The foundational skills branch covers essential capabilities like mathematics, coding, linguistic ability, and reasoning. Compositional skills involve more complex tasks that require a combination of knowledge and foundational skills, such as writing a comprehensive report or solving multi-step problems.

Examples of Task Classification
To illustrate the classification process, consider the following examples:
* Knowledge Branch:
    * Finance: This category includes tasks related to financial analysis, budgeting, and investment strategies. Examples might include summarizing financial reports, explaining investment concepts, and answering domain-specific questions.
    * Statistics: Tasks in this category involve statistical analysis, probability, and data interpretation. Examples include calculating statistical measures, explaining statistical concepts, and interpreting data sets.
* Foundational Skills Branch:
    * Mathematics: This includes tasks such as arithmetic operations, algebra, geometry, and calculus. Examples range from simple addition and subtraction to solving complex equations and understanding geometric principles.
    * Coding: Tasks related to programming and software development. Examples include writing code snippets, debugging programs, and explaining algorithms.
* Compositional Skills Branch:
    * Writing: This involves creating various types of written content, such as emails, reports, and articles. Examples include composing formal emails, drafting technical reports, and writing creative stories.
    * Problem Solving: Tasks that require a combination of knowledge and skills to address complex problems. Examples include solving case studies, performing multi-step calculations, and developing strategic plans.

Rationale Behind Taxonomy Design
The rationale behind the taxonomy design is to ensure comprehensive coverage of potential tasks while maintaining a structured approach to data generation. By categorizing tasks into distinct branches and subcategories, we can target specific areas for improvement and ensure that the synthetic data generation process addresses a wide range of instructional needs.
This structured approach also facilitates the identification of gaps in the training data, allowing for the addition of new tasks as needed. Furthermore, the hierarchical structure ensures that the generated data is both diverse and high-quality, providing a robust foundation for the instruction-tuning phase.

Taxonomy-Driven Synthetic Data Generator
The small number of manually curated data samples, embedded in the leaf nodes of the taxonomy, can be directly used for instruction tuning of the chatbot, however, the model may still perform poorly. Prior work (Li et al., 2023) has shown that typically, a large amount of high-quality instruction data is required for improving instruction following performance of LLMs. It is possible to leverage existing SDGs like Wang et al. (2023); Taori et al. (2023) to use the embedded examples and generate a lot more instruction data synthetically using teacher LLMs. But, such distillation-based SDGs tend to over-sample from the dominant modes of the teacher model and thus lack in diversity and quality of the generated data Gudibande et al. (2023). We argue that this limitation is attributed to the random selection of examples from the pool of seed samples: with random selection, the examples used to prompt the teacher model at each time are an “average” of the seed pool i.e. they do not focus on any specific task. This lack of focus tends to encourage the teacher model to generate more synthetic data from its dominant modes and ignore the long tail of interesting tasks.

Skill Generation
Skills-SDG uses four prompt templates, one for each of the four, below-mentioned, stages of data generation. Each template has its own set of principles and instructions that control the role of the teacher model (generator vs evaluator) and guide the generation/evaluation process.
1. Instruction Generation: In the first stage, the teacher model acts as a question generator, using a specialized prompt (see Figure 3 for an example) to leverage its knowledge and create diverse questions. By iterating through each leaf node of a taxonomy, the teacher generates queries that adhere to specific principles and thoroughly explore the targeted domain, enhancing the comprehensiveness of the generated content.
2. Evaluating Synthetic Instruction: In this stage, the teacher model assumes the role of an instruction evaluator, the teacher model uses targeted prompts to filter out questions that don’t meet predefined principles, including relevance to the domain, potential harm, or questions beyond a language model’s answering capabilities. This ensures that only high-quality, contextually appropriate questions move forward in the process.
3. Generating Responses: The teacher model, functioning as a response generator in this stage, adopts dual personas for precision and creativity, guided by distinct prompts. This tailored approach helps to generate both, creative responses for domains like writing and role-play, and precise answers for STEM and data extraction, aligning the response style to human expectations through principles and seed examples in the leaf nodes.
4. Evaluating the Synthetic Instruction-Response Pair: The final stage involves a rigorous process to filter and select high-quality instruction and response pairs. Using a 3-point rating system (see Figure 4 for an example), the teacher model evaluates each sample, filtering out those that are incorrect, irrelevant, or deviate from the provided principles, ensuring the training dataset’s quality and relevance are enhanced for the student model.

Knowledge-Generation
Synthetic data generators are inherently limited by the knowledge and capabilities of the teacher model. This is one of the main reasons why most successful SDG methods (Xu et al., 2023; Mukherjee et al., 2023; Mitra et al., 2023) depend on GPT-4 model, which presumably has the highest coverage of knowledge and skills. However, there are many domains that no open/proprietary model is trained on and hence cannot work as a teacher model using existing SDG methods. To address this limitation, in LAB we devised a new SDG pipeline for generating instruction data on domains that the teacher model has not been trained on. We call it knowledge-SDG.
Similar to the process of skills generation, knowledge-SDG uses the curator-provided examples embedded in the leaf nodes of the knowledge branch of the taxonomy. But additionally, the teacher model is provided a knowledge source in the form of documents, manuals, and books on the target subject to ground the generated instruction data into a reliable source thus avoiding dependence on the internal knowledge base of a teacher model, which may struggle with specialized domains and could lead to inaccuracies or hallucinations especially on highly specialized, technical domains.
To ensure that the generated answers remain faithful to the content of the source material, similar to the skills-SDG, teacher model is repurposed as an evaluator that validates the generated responses are grounded and faithful to the source documents.

Multi-Phase Training
LAB training happens in two phases, knowledge tuning, followed by skills tuning.
In the knowledge-tuning phase, the model is trained on samples from the knowledge and foundational skills branches of the taxonomy. This phase in-turn, is carried out in two steps. We split the data under the knowledge and foundational skills branches into two buckets based on the response length. Then we first train the model on the samples with short responses before moving on to training on samples with long responses. Similar to prior work (Mitra et al., 2023), our empirical results also suggest that this two-step approach to knowledge-tuning improves model performance.
Post-knowledge tuning, we start the skills-tuning phase where the best model checkpoint from the knowledge-tuning phase is trained on the compositional skills branch of the taxonomy. To address the challenge of catastrophic forgetting when training in two distinct phases, a replay buffer of the data from the knowledge-tuning phase is employed. Our empirical findings indicate that starting with knowledge and foundational skills training, before progressing to compositional skills leads to significantly better benchmark performance.
For selecting the best model checkpoint during intermediate phases, we rely on the MMLU benchmark (Hendrycks et al., 2020) during the knowledge-tuning phase and the MT-bench (Zheng et al., 2024) during the skills-tuning phase. Please refer to table 1 for an overview of our training phases.

Knowledge Tuning
The knowledge-tuning phase is critical for grounding the model in factual and domain-specific information. We split this phase into two steps based on the length of the responses. The first step involves training on samples with short responses, which helps the model learn to provide concise and accurate answers. The second step involves longer responses, allowing the model to handle more complex and detailed queries.

Skill Tuning
In the skills-tuning phase, the focus shifts to enhancing the model's ability to perform tasks that require a combination of knowledge and foundational skills. The best model checkpoint from the knowledge-tuning phase is further trained on the compositional skills branch of the taxonomy. To prevent catastrophic forgetting, we use a replay buffer that includes data from the knowledge-tuning phase. This approach ensures that the model retains its ability to handle foundational tasks while acquiring new skills.

Replay Buffer Mechanism
The replay buffer mechanism is essential for maintaining the model’s performance across different training phases. By continuously incorporating data from previous phases, the model avoids forgetting previously learned tasks. This mechanism allows for a more stable integration of new knowledge and skills, leading to better overall performance.


Results
Benchmark Comparisons
In this study, we implemented the LAB method on two distinct open models, LLAMA-2-13B (Touvron et al., 2023) and MISTRAL-7B (Jiang et al., 2023), utilizing MIXTRAL-8X7B-INSTRUCT-V0.1 (Jiang et al., 2024) as the teacher model. This approach yielded two LAB-aligned models: LABRADORITE-13B and MERLINITE-7B.
During the synthetic data generation phase, we employed a taxonomy consisting of numerous leaf nodes to produce a dataset comprising 1.2 million samples, divided almost evenly between knowledge-based (617k) and skill-based (588k) samples. The specific training hyper-parameters employed during this study are summarized in table 2.
We compare the performance of LABRADORITE-13B and MERLINITE-7B against other models that use the same base models for alignment, which include:
* LLAMA-2-13B
    * LLAMA-2-13B-CHAT (Touvron et al., 2023): RLHF with human annotators by the same team that develops LLAMA-2-13B.
    * ORCA-2 (Mitra et al., 2023): Progressive training with GPT-4.
    * WIZARDLM-13B-V1.2 (Xu et al., 2023): Evol-Instruct with GPT-4.
* MISTRAL-7B
    * MISTRAL-7B-INSTRUCT-V0.2 (Jiang et al., 2023): Instruction-tuning using supervised fine-tuning (SFT) on publicly available conversation datasets.
    * ZEPHYR-7B-BETA (Tunstall et al., 2023): SFT + DPO with GPT-4.
To compare the aligned LLMs, we consider the following evaluation metrics with the settings consistent with those used by LMSYS Chatbot Arena Leaderboard (Zheng et al., 2023):
* MT-Bench (Zheng et al., 2023): 1-turn and 2-turn average.
* MMLU (Hendrycks et al., 2021): 5-shot.
* ARC (Clark et al., 2018): 25-shot.
* HellaSwag (Zellers et al., 2019): 10-shot.
* Winogrande (Sakaguchi et al., 2019): 5-shot.
* GSM8k (Cobbe et al., 2021): 5-shot strict.

Case Studies and Examples
Notably, in terms of MT-Bench, LABRADORITE-13B performs better than the current best model fine-tuned on LLAMA-2-13B and MERLINITE-7B performs better than the current best model fine-tuned on MISTRAL-7B, achieving state-of-the-art performance in terms of chatbot capability. Importantly, our training method ensures that the model is not only good at multi-turn conversation but also maintains its knowledge or reasoning capability, as shown by the overall superior performance in the rest of the metrics. Besides, unlike those top models that use GPT-4 as the teacher model, we achieve this performance using the open-weights MIXTRAL-8X7B-INSTRUCT-V0.1, which is a relatively weaker teacher model at orders of magnitude less cost.

Visualizations and Analysis
To provide a more comprehensive understanding of LAB's performance, we include visualizations such as graphs and tables that illustrate the benchmark metrics. These visualizations help in highlighting the improvements achieved by LAB-trained models across various tasks and benchmarks.


Discussion
Analysis of LAB’s Results
The results demonstrate that LAB-trained models can achieve competitive performance across several benchmarks. By leveraging a taxonomy-guided synthetic data generation process, LAB ensures high data diversity and quality without relying on proprietary models. The multi-phase training framework with replay buffers effectively prevents catastrophic forgetting, allowing for stable integration of new knowledge and skills.

Potential Limitations
While LAB offers significant advantages, there are potential limitations to consider. The reliance on open-source teacher models, while cost-effective, may limit the quality of generated synthetic data compared to proprietary models like GPT-4. Additionally, the taxonomy design and data curation processes require careful planning and continuous updates to address emerging tasks and domains.

Broader Impact on LLMs and NLP
LAB’s approach to large-scale alignment tuning has broader implications for the field of LLMs and NLP. By providing a scalable and cost-effective solution for instruction tuning, LAB enables the development of more robust and versatile language models. This can lead to improvements in various applications, including customer support, content generation, and interactive AI systems.

Conclusion
Summary of Findings
In this paper, we introduced LAB, a novel methodology designed to enhance the scalability and efficiency of the alignment tuning phase for LLMs. LAB leverages a taxonomy-guided synthetic data generation process and a multi-phase training framework with replay buffers to achieve competitive performance across several benchmarks.

Broader Implications
The findings of this study have significant implications for the field of NLP. By reducing the reliance on costly human annotations and proprietary models, LAB offers a scalable and cost-effective solution for training large language models. This approach can be applied to various domains, enabling the development of more versatile and capable LLMs.

Future Research Directions
Future research can explore several directions to build on the findings of this study. Potential areas of investigation include:
* Enhancing the taxonomy design to cover more tasks and domains.
* Improving the quality of synthetic data generation using advanced techniques.
* Exploring alternative training frameworks to further prevent catastrophic forgetting.
* Investigating the applicability of LAB to other types of machine learning models and tasks.
